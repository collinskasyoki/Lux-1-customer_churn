# Introduction
Businesses always seek ways to obtain new customers and keep existing ones subscribed to their services. Telecom companies are among the most concerned about keeping customers. Customer churn is the loosing of customers or subscribers to a service. The need to predict and understand the reasons for customers leaving is critical to creating ways to counter churn. For this fictional scenario, we attempt to build a model to predict customer churn to determine whether, given a set of conditions, we can know whether or not a customer will leave.

The approach utilizes a set of steps to obtain the data, prepare it, and build a model to predict the churn. The dataset is generated by a custom Python script that randomly assigns values based on a set of criteria. The script is accessible [here](https://github.com/collinskasyoki/Lux-1-customer_churn/blob/main/churn.py). 

Please note that this script randomly assigns the values, and there are no patterns that you would typically find in a real-world dataset. The dataset is accessible [here](https://github.com/collinskasyoki/Lux-1-customer_churn/raw/main/data.csv), and contains 2987 records.

# Data Importing and Preprocessing
First we gather the necessary libraries we will utilize for the analysis
```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from datetime import datetime
```

We then import the data
```python
df = pd.read_csv('sprint_customer_data.csv')
df.head()
```
![image](https://github.com/collinskasyoki/Lux-1-customer_churn/assets/40399921/b890eb8b-8f83-4914-9047-b0f47a1f3181)


Here, we drop some unnecessary columns that will not be useful for the analysis.
```python
df = df.drop(columns=['CustomerID', 'Name', 'PhoneNumber', 'EmailAddress', 'City'])
df.head()
```
![image](https://github.com/collinskasyoki/Lux-1-customer_churn/assets/40399921/bf47f5a1-2c7c-4c93-b046-ded41eaa7771)


We then convert the categorical variables into numerical ones. Most ML models work with numerical data.
```python
## Converting PlanType, State, and ZipCode to numerical
le = LabelEncoder()
df['PlanType'] = le.fit_transform(df['PlanType'])
df['State'] = le.fit_transform(df['State'])
df['ZipCode'] = le.fit_transform(df['ZipCode'])
```

We then convert date columns into datetime objects for easy processing with pandas
```python
## Converting date columns to datetime
df['CustomerSince'] = pd.to_datetime(df['CustomerSince'])
df['LastInteraction'] = pd.to_datetime(df['LastInteraction'])
```

Finally, we convert the dates for LastInteractions and CustomerSince variables into months till the current date. The aim is to allow the ML model to look into whether the longevity of customer interaction and subscription period is related to their leaving.
```python
## Calculating total months since customers joined and last interacted with the company
df['MonthsCustomerSince'] = (datetime.now() - df['CustomerSince']).dt.days // 30
df['MonthsSinceLastInteraction'] = (datetime.now() - df['LastInteraction']).dt.days // 30
```

# Building the model
Choosing the features and target variable
```python
## Features vs target variable
x = df[['Age', 'MonthlyBill', 'DataUsageGB', 'ContractMonths', 'DevicesOwned', 'State', 'PlanType', 'TotalSpent', 'MonthsCustomerSince', 'MonthsSinceLastInteraction']]
y = df['Exited']
```

We then split the dataset into test and training sub-sets. Not that we chose an 80/20 split. Also, note the random_state attribute to control the randomness of the choices and allow reproducible results.
```python
# Split to train & test sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=30)
```

## Logistic Model
```python
model = LogisticRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
print(classification_report(y_test, y_pred))
```
![image](https://github.com/collinskasyoki/Lux-1-customer_churn/assets/40399921/e5b6dd69-19f9-4544-b284-92e8829eeff4)


## XGBoost Model
```python
model2 = XGBClassifier()
model2.fit(x_train, y_train)
y2_pred = model2.predict(x_test)
print(classification_report(y_test, y2_pred))
```
![image](https://github.com/collinskasyoki/Lux-1-customer_churn/assets/40399921/bab893e3-b49f-4919-b70c-acfb01ac120b)

# Conclusion
The accuracy for the Logistic Regression Model is 50%, while that of XGBoost is 49%. According to our case study, this is a poor performance for both models. This is because it is no different from attributing the leaving of customers to chance. For a model that aims to predict customer churn, we typically need a better-performing model. The precision and recall values are also near 50%, further showing the poor performance in this scenario. There is an emphasis on the scenario whereby a 50% prediction chance is not enough since it is no different from random chance. A different case predicting different attributes might consider a 50% accuracy as good enough.
That said, the poor performance can be attributed to the random generation of the data. Since the script is not generating real-world data, it shows how well-balanced the data is towards randomness. A set of real-world customer data would bring different results.
